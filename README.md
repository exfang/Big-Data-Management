# Big Data Management

# Aim
With more and more industry storing their data on the cloud, the skill and knowledge to analyze these data from the cloud directly is getting more important. Through this WIU, learners will demonstrate their competencies in analyzing Big Data to arrive at an outcome. Learners will be able to identify and access big data analytics-related business problems and propose a viable big data analytics solution to address the business issues. Learners will also be able to evaluate the effectiveness of the analytics solutions in terms of predictive accuracies as well as system scalability.

# Objective

1. Collect and analyze big data using research frameworks and historical data that comply with data privacy and ethics.
2. Perform big data processing and aggregation operations on big data in the cloud platform using data manipulation language to address business needs.
3. Perform data visualization and data analysis on the dataset from the cloud.
4. 4. Deploy their project on a cloud platform.
5. Work collaboratively in a team to develop dashboards using a data storytelling approach for an effective narrative and visual representation.

## Work Done

1. Dataset sourcing from Kaggle (https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease)
2. Data Understanding and Preparation.
3. Train Machine Learning models for predictions (Heart Disease, Diabetes, Stroke, and Skin Cancer).
4. Automation of patient disease prediction on streaming data on the Databricks platform (Batch streaming in this project).

## Members

- Andrew (Group Leader, in charge of Heart Disease prediction)
- Charmaine (Member, in charge of Skin Cancer prediction)
- Joseph (Member, in charge of Stroke prediction)
- Nicole (Member, in charge of Diabetic prediction)

## Guide to running batch streaming notebook

1. Set up an account within the Databricks community edition.
2. Create a compute cluster.
3. Upload the streaming file onto the workspace (group final files > Streaming on databricks.dbc)
4. Connect the compute cluster to the streaming file.
5. Locate the cleaned dataset (group final files > dataset for training and streaming folder > heart_2020_WOstream.csv)
6. Upload the dataset (catalog > create table > upload dataset)
7. Run the notebook.
8. Catalog > Create Table > Select > stream_read > Upload the streaming data and view prediction output on the notebook/ in the stream_prediction folder.

## View Live Demo of the batch streaming on YouTube.

[https://studio.youtube.com/video/J93pE4znDI4/edit](https://www.youtube.com/watch?v=J93pE4znDI4&feature=youtu.be)https://www.youtube.com/watch?v=J93pE4znDI4&feature=youtu.be

